{% extends "tutorial.html" %}

{% block pagebreadcrumb %}{{ tut.title }}{% endblock %}

{% block html5badge %}
<img src="/static/images/identity/html5-badge-h-multimedia.png" width="133" height="64" alt="This article is powered by HTML5 Audio/Video" title="This article is powered by HTML5 Audio?/Video" />
{% endblock %}

{% block iscompatible %}
return !! (window.PeerConnection || window.webkitDeprecatedPeerConnection || window.webkitPeerConnection00);
{% endblock %}

{% block head %}
<style>
</style>
{% endblock %}

{% block onload %}
// TODO
{% endblock %}

{% block content %}

<h2 id="toc-introduction">Introduction</h2>

<blockquote>WebRTC is a new front in the long war for an open and unencumbered web.<br />
&mdash; <a href="http://hacks.mozilla.org/2012/03/video-mobile-and-the-open-web/" title="Brendan Eich blog post: Video, Mobile, and the Open Web">Brendan Eich</a>, inventor of JavaScript
</blockquote>


<p>Imagine a world where your phone, TV and computer could all communicate on a common platform. Imagine it was easy to build text chat or video and voice calling into your web application. That's the vision of WebRTC.</p>

<p>WebRTC implements open standards for real-time, plugin-free video, audio and data communication. The need is real:
  <ul>
    <li>A lot of web services already use Real-time Communication (RTC), but with plugins. These includes Skype, Facebook (which uses Skype) and Google Hangouts (which use the Google Talk plugin).</li>
    <li>Plugin download, installation and update can be complex, error prone and annoying.</li>
    <li>For developers, plugins can be difficult to deploy, update, debug, troubleshoot, test and maintain. It can be hard to persuade people to install plugins in the first place!</li>
  </ul>
</p>

<p>The guiding principles of the WebRTC project are that its APIs should be open source, free, standardised, and more efficient than existing technologies.</p>

<p>Want to try it out? WebRTC is available now in Google Chrome.</p>

<!-- 
<p>A good place to start with WebRTC is the simple video chat application at <a href="http://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a>. In Chrome, open the page (with PeerConnection enabled on the chrome://flags page) and you'll notice that it redirects to a URL such as <code>https://apprtc.appspot.com/?r=18902167</code>, which includes a query string giving the randomly generated 'room number'. Open that URL in another tab or window (preferably on another computer!) to start video chat. If you open the page again, make sure to use the bare <a href="http://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a> URL, without the query string such as <code>/?r=18902167</code> appended: the app must get a fresh room number every time.</p>
 -->
 
 <p>A good place to start is the simple video chat application at <a href="http://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a>. Open the page in Chrome, with PeerConnection enabled on the chrome://flags page, then open the URL again (with query string added) in a new window. There is a walkthrough of the code <a href="#toc-simple" title="Code walkthrough of apprtc.appspot.com">later in this article</a>.</p>


<h2 id="toc-history">A very short history of WebRTC</h2>

<p>For many years, RTC tools were expensive and complex&mdash;out of the reach of individuals and smaller companies. </p>

<p>Gmail video chat became popular in 2008, and in 2011 Google introduced Hangouts, which use the same plugin as Gmail. Google bought GIPS, a company which had developed many of the components required for RTC, such as codecs and echo cancellation techniques. Google open sourced the technologies developed by GIPS (around $70 million worth) and engaged with relevant standards bodies, the IETF and W3C, to ensure industry consensus. In May 2011, Ericsson built <a href="https://labs.ericsson.com/developer-community/blog/beyond-html5-peer-peer-conversational-video" title="Beyond HTML5: peer to peer conversational video">the first implementation of WebRTC</a>.</p>

<p>APIs used by WebRTC apps, such as <a href="http://www.html5rocks.com/en/tutorials/getusermedia/intro/" title="HMTL5 Rocks: Capturing Audio & Video in HTML5" target="_blank">getUserMedia</a> and <a href="http://www.html5rocks.com/en/tutorials/websockets/basics/" title="HTML5 Rocks: Introducing WebSocket">WebSocket</a>, emerged at the same time. Future integration with APIs such as <a href="http://www.html5rocks.com/en/tutorials/webaudio/intro/" title="HTML5 Rocks: Getting Started With Web Audio">Web Audio</a> will make WebRTC even more powerful&mdash;WebRTC has already <a href="https://webglmeeting.appspot.com" title="WebGL WebRTC demo">shown huge promise</a> when teamed up with technologies such as <a href="http://learningwebgl.com" title="Learning WebGL">WebGL</a>.</p>

<h2 id="toc-where">Where are we now?</h2>

<p>WebRTC has been available in the stable build of Google Chrome since version 20. The getUserMedia API has been 'flagless' (you don't have to enable MediaStream on the chrome://flags page) since version 21.</p>

<p>Opera 12 shipped with getUserMedia; further WebRTC implementation is planned for Opera this year. Firefox has WebRTC efforts <a href="http://hacks.mozilla.org/2012/04/webrtc-efforts-underway-at-mozilla/" title="Firefox roadmap 2012">underway</a>, and has demonstrated a prototype version of PeerConnection. Full getUserMedia support <a href="http://mozillamediagoddess.org/2012/06/25/the-first-part-of-webrtc-has-landed/" title="mozillamediagoddess blog post">is planned</a> for Firefox 17 on desktop and Android. WebRTC functionality is available in Internet Explorer <a href="https://groups.google.com/forum/#!topic/discuss-webrtc/tKoh1wrI8ig" title="How to enable WebRTC functionality in Internet Explorer via Chrome Frame">via Chrome Frame</a>, and Skype (acquired by Microsoft in 2011) is reputedly <a href="http://gigaom.com/2012/06/26/skype-webrtc-web-client/" title="GigaOM blog post about Skype and WebRTC">planning to use WebRTC</a>. Native implementations with WebRTC include <a href="https://labs.ericsson.com/developer-community/blog/beyond-html5-conversational-voice-and-video-implemented-webkit-gtk" title="Ericsson article about WebKitGTK+">WebKitGTK+</a>. Mobile WebRTC support is planned for later this year.</p>

<p>As well as the browser vendors, WebRTC has strong support from Cisco, Ericsson and other companies such as Voxeo, who recently announced the <a href="http://phono.com/webrtc" title="Phono SDK">Phono</a> jQuery plugin for building WebRTC-enabled web apps with phone functionality and messaging.</p>

<p>A word of warning: be skeptical of reports that a platform 'supports WebRTC'. Often this actually just means that getUserMedia is supported, but not any of the other RTC components.</p>

<h2 id="toc-first">My first WebRTC</h2>

<p>WebRTC client applications need to do several things:</p>

<ul>
  <li>Get streaming audio, video or data.</li>
	<li>Communicate streaming audio, video or data.</li>
	<li>Exchange control messages to initiate or close sessions and report errors.</li>
	<li>Exchange information about media such as resolution and format.</li>
</ul>

<p>More specifically, WebRTC as implemented uses the following APIs.</p>
<ul>
	<li><a href="https://dvcs.w3.org/hg/audio/raw-file/tip/streams/StreamProcessing.html" title="MediaStream API documentation">MediaStream</a>: get access to data streams, such as from the user's camera and microphone.</li>
	<li><a href="http://www.webrtc.org/reference/api-description" title="PeerConnection API">PeerConnection</a>: audio or video calling, with facilities for encryption and bandwidth management.</li>
	<li><a href="http://dev.w3.org/2011/webrtc/editor/webrtc.html#peer-to-peer-data-api" title="W3C WebRTC DataChannel Editor's draft">DataChannel</a>: peer-to-peer communication of generic data.</li>
</ul>

<h2 id="toc-streams">Crossing the streams</h2>

<p>The MediaStream API represents a source of streaming media. Each MediaStream has one or more MediaStreamTracks, each of which corresponds to a synchronised media source. For example, a stream taken from camera and microphone input would have synchronised video and audio tracks. (Don't confuse MediaStream tracks with the &lt;track&gt; element, which is something <a href="http://www.html5rocks.com/en/tutorials/track/basics/" title="HTML5 Rocks: Getting Started With the Track Element">entirely different</a>.)</p>

<p>The <code>getUserMedia()</code> function can be used to get a LocalMediaStream, which has a <code>label</code> identifying the source device (something like 'FaceTime HD Camera (Built-in)') as well as <code>audioTracks</code> and <code>videoTracks</code> properties, each of which is a MediaStreamTrackList. In Chrome, the <code>webkitURL.createObjectURL()</code> method converts a LocalMediaStream to a <a href="http://www.html5rocks.com/tutorials/workers/basics/#toc-inlineworkers-bloburis" title="HTML5 Rocks: Blob URLs">Blob URL</a> which can be set as the <code>src</code> of a video element. (In Opera, the <code>src</code> of the video can be set from the stream itself.)</p>

<p>Note that currently no browser allows audio data from getUserMedia to be passed to an audio or video element, or to other APIs such as Web Audio: the WebRTC PeerConnection API handles audio as well as video, but audio from getUserMedia is not yet supported in other contexts.</p>

<p>You can try out getUserMedia with the code below, if you have a webcam. Paste the code into the console in Chrome and press return. Permission to use the camera and microphone will be requested in an infobar at the top of the browser window; press the Allow button to proceed. The video stream from the webcam will then be displayed in the video element created by the code.</p>

<pre class="prettyprint">
// try this in Chrome
navigator.webkitGetUserMedia({video:true}, function(localMediaStream) { 
  var video = document.createElement("video");
  video.autoplay = true;
  video.src = window.webkitURL.createObjectURL(localMediaStream);
  document.body.innerHTML = video.outerHTML; // could use replaceChild()
}, function(error) {
  console.log(error);
});</pre>

<p>Here's a non-prefixed version of the same code, which will work in browsers such as Opera that support it:</p>

<pre class="prettyprint">
// try this in Opera
navigator.getUserMedia({video:true}, function(localMediaStream) { 
  var video = document.createElement("video");
  video.autoplay = true;
  video.src = localMediaStream; // no need to create Blob URL
  document.body.parentNode.replaceChild(video, document.body); 
}, function(error) {
  console.log(error);
});
</pre>

<p>The intention is eventually to allow MediaStreams for any streaming data source, not just a camera or microphone. This could be extremely useful for gathering real-time data, for example from sensors or other input devices.</p>

<h2 id="toc-signalling">Signalling</h2>

<p>WebRTC uses PeerConnection to communicate streams of data, but also needs a mechanism to send control messages between peers, a process known as signalling. Signalling methods and protocols are <em>not</em> specified by WebRTC: signalling is not part of the PeerConnection API. Instead, WebRTC app developers can choose whatever messaging protocol they prefer, such as SIP or XMP, and any appropriate duplex (two-way) communication channel such as WebSocket, or XMLHttpRequest (XHR) in tandem with the <a href="https://developers.google.com/appengine/docs/python/channel/overview" title="Google Channel API documentation">Google Channel API</a>. </p> 

  <p>The <a href="http://apprtc.appspot.com" title="apprtc WebRTC example">apprtc.appspot.com</a> example uses XHR and the Channel API. Silvia Pfeiffer has demonstrated <a href="http://blog.gingertech.net/2012/06/04/video-conferencing-in-html5-webrtc-via-web-sockets/" title="Silvia Pfeiffer blog post: WebRTC via WebSocket">WebRTC signalling via WebSocket</a> and in May 2012 Doubango Telecom open-sourced the <a href="http://sipml5.org/" title="sipml5 site">sipml5 SIP client</a>, built with WebRTC and WebSocket.</p>

<p>To start a session, WebRTC clients need the following:</p>

<ul>
	<li>Local configuration information.</li>
	<li>Remote configuration information.</li>
	<li>Remote transport candidates: how to connect to the remote client (IP addresses and ports).</li>
</ul>

<p>Configuration information is described in the form of a <strong>SessionDescription</strong>. the structure of which conforms to the <a href="http://en.wikipedia.org/wiki/Session_Description_Protocol" title="Wikipedia article about the Session Description Protocol">Session Description Protocol</a>, SDP. Serialised, an SDP object looks like this:</p>

<pre class="prettyprint">
v=0
o=- 3883943731 1 IN IP4 127.0.0.1
s=
t=0 0
a=group:BUNDLE audio video
m=audio 1 RTP/SAVPF 103 104 0 8 106 105 13 126

// ...

a=ssrc:2223794119 label:H4fjnMzxy3dPIgQ7HxuCTLb4wLLLeRHnFxh810
</pre>

<p>Signalling proceeds like this:</p>

<ol>
	<li>Caller sends offer.</li>
	<li>Callee receives offer.</li>
	<li>Callee sends answer.</li>
	<li>Caller receives answer.</li>
</ol>

<p>The SessionDescription sent by the caller is known as an <strong>offer</strong>, and the response from the callee is an <strong>answer</strong>. (Note that WebRTC currently only supports one-to-one communication.)</p>

<p>The offer SessionDescription is passed to the caller's browser via the PeerConnection <code>setLocalDescription()</code> method, and via signalling to the remote peer, whose own PeerConnection object invokes <code>setRemoteDescription()</code> with the offer. This architecture is called <a href="http://tools.ietf.org/html/draft-ietf-rtcweb-jsep-00" title="IETF JSEP draft proposal">JSEP</a>, JavaScript Session Establishment Protocol. (There's an excellent animation explaining the process of signalling and streaming in <a href="https://labs.ericsson.com/developer-community/blog/beyond-html5-peer-peer-conversational-video" title="Ericsson conversational video demo">Ericsson's demo video</a> for its first WebRTC implementation.) </p>

<p>Just to reiterate: signalling messages are communicated via whatever mechanism the developer chooses: the signalling mechanism is not specified by WebRTC.</p>

<p>Once the signalling process has completed successfully, data can be streamed directly, peer to peer, between the caller and callee. Streaming is the job of PeerConnection.</p>

<h2>PeerConnection</h2>

<p>Below is a WebRTC architecture diagram. As you will notice, the green parts are complex!</p>

<figure>
<a href="http://www.webrtc.org/reference/architecture" title="webrtc.org: architecture diagram"><img src="http://www.webrtc.org/_/rsrc/1317202919504/reference/WebRTCpublicdiagramforwebsite%20%282%29.png" alt="WebRTC architecture diagram" style="width: 740px; height: 482px;" /></a>
<figcaption>WebRTC architecture (from <a href="http://www.webrtc.org/reference/architecture" title="webrtc.org: architecture diagram">webrtc.org</a>)</figcaption>
</figure>

<p>The main thing to understand from this diagram is that the underlying code does a lot of work, but PeerConnection shields web developers from the myriad complexities that lurk beneath.</p>

<h3 id="toc-sans">PeerConnection sans signalling</h3>

<p>Streaming setup from the PeerConnection point of view is described in the code example below. The code is taken from the 'single page' example at <a href="https://webrtc-demos.appspot.com/html/pc1.html" title="WebRTC demo without signalling">webrtc-demos.appspot.com</a>, which implements caller and callee on the same web page: local and remote PeerConnection and local and remote video, all in the same block of code. This doesn't constitute a very useful app, but makes it possible to demonstrate PeerConnection without the complexities of signalling.</p>

<p>First, a quick explanation of the name <strong>webkitPeerConnection00</strong>. When PeerConnection using the JSEP architecture (see above) was implemented in Chrome (see above), the original pre-JSEP implementation was renamed webkitDeprecatedPeerConnection. This made it possible to keep old demos working with a simple rename. The new JSEP PeerConnection implementation was named webkitPeerConnection00, and as the JSEP draft standard evolves, it might become webkitPeerConnection01, webkitPeerConnection02&mdash;and so on&mdash;to avoid more breakage. When the dust finally settles, the API name will become PeerConnection.</p>

<h3>Caller</h3>

<ol>

<li>
<p>Create a new PeerConnection and add a stream (for example, from a webcam):</p>
<pre class="prettyprint">
pc1 = new webkitPeerConnection00(null, iceCallback1);
// ...
pc1.addStream(localstream);</pre>
</li>

<li>
<p>Create a local SessionDescription, apply it and initiate a session:</p>
<pre class="prettyprint">
var offer = pc1.createOffer(null);
pc1.setLocalDescription(pc1.SDP_OFFER, offer);
// ...
pc1.startIce(); // start connection process
</pre>
</li>

<li><p>(Wait for a response from the callee.)</p></li>

<li>
<p>Receive remote SessionDescription and use it:</p>
<pre class="prettyprint">
pc1.setRemoteDescription(pc1.SDP_ANSWER, answer);
</pre>
</li>

</ol>

<h3>Callee</h3>

<ol>

<li><p>(Receive call from caller.)</p></li>

<li>
<p>Create PeerConnection and set description:</p>
<pre class="prettyprint">
pc2 = new webkitPeerConnection00(null, iceCallback2);
pc2.onaddstream = gotRemoteStream; 
// ...
pc2.setRemoteDescription(pc2.SDP_OFFER, offer);
</pre>
</li>

<li>
<p>Create local SessionDescription, apply it, and kick off response:</p>
<pre class="prettyprint">
var answer = pc2.createAnswer(offer.toSdp(), 
  {has_audio:true, has_video:true});
// ...
pc2.setLocalDescription(pc2.SDP_ANSWER, answer);
pc2.startIce();
</pre>
</li>

</ol>

<p>Here's the whole process (sans logging):</p>

<pre class="prettyprint">
// create the 'sending' PeerConnection
pc1 = new webkitPeerConnection00(null, iceCallback1);
// create the 'receiving' PeerConnection
pc2 = new webkitPeerConnection00(null, iceCallback2);
// set the callback for the receiving PeerConnection to display video
pc2.onaddstream = gotRemoteStream; 
// add the local stream for the sending PeerConnection
pc1.addStream(localstream);
// create an offer, with the local stream
var offer = pc1.createOffer(null);
// set the offer for the sending and receiving PeerConnection
pc1.setLocalDescription(pc1.SDP_OFFER, offer);
pc2.setRemoteDescription(pc2.SDP_OFFER, offer);
// create an answer
var answer = pc2.createAnswer(offer.toSdp(), {has_audio:true, has_video:true});
// set it on the sending and receiving PeerConnection 
pc2.setLocalDescription(pc2.SDP_ANSWER, answer);
pc1.setRemoteDescription(pc1.SDP_ANSWER, answer);
// start the connection process
pc1.startIce();
pc2.startIce();
</pre>

<h2 id="toc-real">PeerConnection plus signalling</h2>

<p>So... That's WebRTC on one page in one browser. But what about a real application, with peers on different computers?</p> 

<p>In the real world, WebRTC needs servers, however simple, so the following can happen:</p>

<ul>
	<li>Users discover each other.</li>
	<li>Users send their details to each other.</li>
	<li>Communication survives network glitches.</li>
	<li>WebRTC client applications communicate data about media such as video format and resolution.</li>
	<li>WebRTC client applications traverse <a href="http://en.wikipedia.org/wiki/NAT_traversal" title="Wikipedia article: Network Address Translation traversal">NAT gateways</a>.</li>
</ul>

<p>In a nutshell, WebRTC needs two types of server-side functionality:</p>
<ul>
	<li>A server app to enable user discovery, communication and signalling.</li>
	<li>A server to handle NAT traversal and other streaming and communication requirements.</li>
</ul>

<p>NAT traversal, and the requirements for building a server app for user discovery and signalling, are beyond the scope of this article. Suffice to say that the <a href="http://en.wikipedia.org/wiki/STUN" title="Wikipedia STUN article">STUN</a> protocol and its extension <a href="http://en.wikipedia.org/wiki/Traversal_Using_Relay_NAT" title="Wikipedia article about TURN">TURN</a> are used by the <a href="http://en.wikipedia.org/wiki/Interactive_Connectivity_Establishment" title="Wikipedia article about ICE">ICE</a> framework to enable PeerConnection to cope with NAT traversal.</p>

<p>The server source code for the <a href="http://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a> example below is available from <a href="http://code.google.com/p/webrtc-samples/source/browse/trunk/apprtc/" title="Code for apprtc.appspot.com demo">code.google.com/p/webrtc-samples/source/browse/trunk/apprtc/</a>&mdash;and check out Silvia Pfeiffer's <a href="http://blog.gingertech.net/2012/06/04/video-conferencing-in-html5-webrtc-via-web-sockets/" title="Silvia Pfeiffer blog post: WebRTC via WebSocket">WebSocket WebRTC app</a>.</p>

<h2 id="toc-simple">A simple video chat client</h2>

<p>As mentioned above, a good place to try out WebRTC is <a href="http://apprtc.appspot.com" title="Simple WebRTC demo" target="_blank">apprtc.appspot.com</a>.</p>

<p>The code in this demo is deliberately verbose in its logging: the console is a good place to understand the order of events.</p>

<p>Below we give a detailed walk-through of the code.</p>

<h3>What's going on?</h3>

<p>The demo starts by running the <code>initalize()</code> function:</p>

<pre class="prettyprint">
function initialize() {
  console.log("Initializing; room=82903889.");
  card = document.getElementById("card");
  localVideo = document.getElementById("localVideo");
  miniVideo = document.getElementById("miniVideo");
  remoteVideo = document.getElementById("remoteVideo");
  resetStatus();
  openChannel();
  getUserMedia();
}
</pre>

<p>This code initializes variables for the HTML video elements that will display video streams from the local camera (<code>localVideo</code>) and from the camera on the remote client (<code>remoteVideo</code>). <code>resetStatus()</code> simply sets a status message.</p>

<p>The <code>openChannel()</code> function sets up messaging between WebRTC clients:</p>

<!-- maybe delete code -->

<pre class="prettyprint">
function openChannel() {
  console.log("Opening channel.");
  var channel = new goog.appengine.Channel('AHRlWrqwxKQHdOiOaux3JkDQaxmTvdlYgz1wL69DE20mE3Xq0WaxE3zznRLD6_jwIGiRFlAR-En4lAlLHWRKk862_JTGHrdCHaoTuJTCw8l6Cf7ChMWiVjU');
  var handler = {
    'onopen': onChannelOpened,
    'onmessage': onChannelMessage,
    'onerror': onChannelError,
    'onclose': onChannelClosed
  };
  socket = channel.open(handler);
}
</pre>

<p>For signalling, this demo uses the Google App Engine <a href="http://code.google.com/appengine/docs/python/channel/overview.html" title="Channel API Overview (Python)" target="_blank">Channel API</a>, which enables messaging between JavaScript clients without polling. (WebRTC signalling is covered in more detail <a href="#toc-signalling" title="WebRTC signalling">above</a>).</p>

<p>Establishing a channel in the example works like this:</p>
<ol>
	<li>Client A generates a unique ID.</li>
	<li>Client A requests a Channel token from the App Engine app, passing its ID.</li>
	<li>App Engine app requests a channel and a token for the client's ID from the Channel API.</li>
	<li>App sends the token to Client A.</li>
	<li>Client A opens a socket and listens on the channel set up on the server.</li>
</ol>

<p>Sending a message works like this:</p>
<ul>
	<li>Client B makes a POST request to the App Engine app with an update.</li>
	<li>The App Engine app passes a request to the channel.</li>
	<li>The channel carries a message to Client A.</li>
	<li>Client A's onmessage callback is called.</li>
</ul>

<p>After the call to <code>openChannel()</code>, the <code>getUserMedia()</code> function called by <code>initialize()</code> checks if the browser supports the <code>getUserMedia</code> API. (Find out more about getUserMedia on <a href="http://www.html5rocks.com/en/tutorials/getusermedia/intro/" title="HMTL5 Rocks: Capturing Audio & Video in HTML5" target="_blank">HTML5 Rocks</a>.) If all is well, onUserMediaSuccess is called:

<pre class="prettyprint">
function onUserMediaSuccess(stream) {
  console.log("User has granted access to local media.");
  var url = webkitURL.createObjectURL(stream);
  localVideo.style.opacity = 1;
  localVideo.src = url;
  localStream = stream;
  if (initiator) maybeStart();
}
</pre>

<p>This causes video from the local camera to be displayed in the <code>localVideo</code> element, by creating an <a href="http://www.html5rocks.com/tutorials/workers/basics/#toc-inlineworkers-bloburis" title="HTML5 Rocks: information about Blob URLs">object (Blob) URL</a> for the camera's data stream and then setting that URL as the <code>src</code> for the element. (<code>createObjectURL</code> is used here as a way to get a URI for an 'in memory' binary resource, i.e. a video data stream.) The data stream is also set as the value of <code>localStream</code>, which is subsequently made available to the remote user.</p>

<p>At this point, <code>initiator</code> has been set to 1 (and it stays that way until the session is terminated) so <code>maybeStart()</code> is called:</p>

<pre class="prettyprint">
function maybeStart() {
  if (!started && localStream && channelReady) {
    setStatus("Connecting...");
    console.log("Creating PeerConnection.");
    createPeerConnection();
    console.log("Adding local stream.");
    pc.addStream(localStream);
    started = true;
  }
}
</pre>

<p>This function uses a handy construct when working with multiple asynchronous callbacks: <code>maybeStart()</code> may be called by any one of several functions, but the code in it is run only when <code>localStream</code> has been defined <em>and</em> <code>channelReady</code> has been set to true <em>and</em> communication hasn't already started. So&mdash;if a connection hasn't already been made, and a local stream is available, and a channel is ready for signalling, a connection is created and passed the local video stream. Once that happens, <code>started</code> is set to true, so a connection won't be started more than once.</p>

<h3>PeerConnection</h3>

<p><code>createPeerConnection()</code> is where the real action takes place&mdash;though most of the code is simply to handle different implementations of PeerConnection:</p>

<pre class="prettyprint">
function createPeerConnection() {
  try {
    pc = new webkitDeprecatedPeerConnection("STUN stun.l.google.com:19302",
      onSignalingMessage);
    console.log("Created webkitDeprecatedPeerConnnection with config \"STUN stun.l.google.com:19302\".");
  } catch (e) {
    console.log("Failed to create webkitDeprecatedPeerConnection, exception: " + e.message);
    try {
      pc = new webkitPeerConnection("STUN stun.l.google.com:19302", 
        onSignalingMessage);
      console.log("Created webkitPeerConnnection with config \"STUN stun.l.google.com:19302\".");
    } catch (e) {
      console.log("Failed to create webkitPeerConnection, exception: " + e.message);
      alert("Cannot create PeerConnection object; Is the 'PeerConnection' flag enabled in about:flags?");
      return;
    }
  }
  pc.onconnecting = onSessionConnecting;
  pc.onopen = onSessionOpened;
  pc.onaddstream = onRemoteStreamAdded;
  pc.onremovestream = onRemoteStreamRemoved;
}
</pre>

<p>The underlying purpose is to set up a connection using a STUN server, and to set handlers for each of the PeerConnection events: when a signalling message is received, when a session is connecting or open, and when a remote stream is added or removed. In fact these handlers only log status messages, except for <code>onRemoteStreamAdded()</code>, which sets the source for the <code>remoteVideo</code> element:</p>

<pre class="prettyprint">
function onRemoteStreamAdded(event) {
  console.log("Remote stream added.");
  var url = webkitURL.createObjectURL(event.stream);
  miniVideo.src = localVideo.src;
  remoteVideo.src = url;
  waitForRemoteVideo();  
}
</pre>

<p>(Display of the initially-hidden <code>remoteVideo</code> element is handled by <code>waitForRemoteVideo()</code>.)</p>

<h3>Signalling</h3>

<p>When the <code>PeerConnection</code> object is created, it's passed a handler for signalling messages, which in turn calls <code>sendMessage()</code> to make an XHR request:</p>

<pre class="prettyprint">
pc = new webkitPeerConnection("STUN stun.l.google.com:19302",
 onSignalingMessage);

// ...

function onSignalingMessage(message) {
  sendMessage('/message', message);
}

// ...

function sendMessage(path, message) {
  console.log('C->S: ' + message);
  path += '?r=82903889' + '&u=29967892';
  var xhr = new XMLHttpRequest();
  xhr.open('POST', path, true);
  xhr.send(message);
}

</pre>

<p>For signalling, this demo uses the Google App Engine Channel API. Messages from the API are handled like this:</p>

<pre class="prettyprint">
function onChannelMessage(message) {
  console.log('S->C: ' + message.data);
  if (message.data != 'BYE') {
    if (message.data.indexOf("\"ERROR\"", 0) == -1) {
      if (!initiator && !started) maybeStart();
      <strong>pc.processSignalingMessage(message.data)</strong>;
    }
  } else {
    onRemoteHangup();
  }
}
</pre>

<p>If the message isn't a signal to hang up ('BYE') or an error (i.e. contains the string 'ERROR'), the function calls the PeerConnection <code>processSignalingMessage()</code> method.</p>

<h2 id="toc-datachannel">DataChannel</h2>

<p>The DataChannel API is planned to ship in Google Chrome in late 2012. The API will enable peer-to-peer exchange of arbitrary data with low latency and high throughput.</p>

<p>There are many potential use cases for the API, including:</p>
<ul>
	<li>Gaming</li>
	<li>Remote desktop applications</li>
	<li>Real-time text chat</li>
	<li>File transfer</li>
	<li>Decentralized networks</li>
</ul>

<p>The API has several features to make the most of PeerConnection and enable powerful and flexible peer-to-peer communication:</p>
<ul>
	<li>Leverage PeerConnection session setup.</li>
	<li>Multiple simultaneous channels, with prioritization.</li>
	<li>Reliable and unreliable delivery semantics.</li>
	<li>Built-in security (DTLS) and congestion control.</li>
	<li>Can be used with or without audio or video.</li>
</ul>

<p>The syntax is somewhat similar to WebSocket, with <code>send()</code> and <code>onmessage</code>, as you will see in the code sample below:</p>

<pre class="prettyprint">
// PeerConnection setup and offer-answer exchange omitted  
var dc1 = pc1.createDataChannel("mylabel");  // create the sending DataChannel (reliable mode)
var dc2 = pc2.createDataChannel("mylabel");  // create the receiving DataChannel (reliable mode)

// append received DataChannel messages to a textarea
var receiveTextarea = document.querySelector("textarea#receive");
dc2.onmessage = function(event) { 
  receiveTextarea.value += event.data;
};  

var sendInput = document.querySelector("input#send");
// send message over the DataChannel
function onSend() {
  dc1.send(sendInput.value); 
}
</pre>
<!-- conclusion -->

<h2 id="toc-conclusion">In conclusion</h2>

<p>The APIs and standards of WebRTC can democratise and decentralise tools for content creation and communication&mdash;for telephony, gaming, video production, music making, news gathering and many other applications. Technology doesn't get much more disruptive than this.</p>

<p>We look forward to seeing what inventive developers make of WebRTC as it becomes widely implemented over the next few months. As blogger Phil Edholm <a href="http://www.nojitter.com/post/232901042/webrtc-is-it-a-game-changer" title="nojitter blog post: WebRTC: Is it a Game Changer?">put it</a>, 'Potentially, WebRTC and HTML5 could enable the same transformation for real-time communications that the original browser did for information.'</p>

<h2 id="toc-more">Learn more</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=E8C8ouiXHHk" title="Video of Justin Uberti WebRTC session at Google I/O, 27 June 2012">Video of Justin Uberti's WebRTC session at Google I/O, 27 June 2012</a></li>
  <li><a href="http://www.webrtc.org/" title="webrtc.org">webrtc.org</a>: the home for all things WebRTC&mdash;demos, documentation and discussion.</li>
  <li><a href="http://www.webrtc.org/running-the-demos" title="webrtc.org: running demos in Chrome">webrtc.org demo page</a>: links to demos, and instruction on how to configure Google Chrome Canary for WebRTC.</li>
  <li><a href="https://groups.google.com/forum/?fromgroups#!forum/discuss-webrtc" title="discuss-webrt Google Group">discuss-webrtc</a>: Google Group for WebRTC discussion.</li>
  <li><a href="http://dev.w3.org/2011/webrtc/editor/webrtc.html" title="W3C Editor's Draft document">The WebRTC W3C Editor's Draft</a>.</li>
  <li><a href="http://tools.ietf.org/wg/rtcweb/charters" title="IETF Working Group Charter">IETF Working Group Charter</a>.</li>
  <li><a href="http://tools.ietf.org/html/draft-jesup-rtcweb-data-protocol-01" title="IETF DataChannel documentation">IETF WebRTC Data Channel Protocol Draft</a>.</li>
  <li><a href="http://tools.ietf.org/html/draft-uberti-rtcweb-jsep-02" title="IETF JSEP documentation">IETF JSEP Draft</a>.</li>
  <li><a href="http://tools.ietf.org/html/rfc5245" title="IETF proposed standard for ICE">IETF proposed standard for ICE</a></li>
</ul>

<h2 id="toc-support">WebRTC support summary</h2>

<h3>MediaStream and getUserMedia</h3>
<ul>
	<li>Chrome 21</li>
	<li>Opera, Opera Mobile 12</li>
	<li>Firefox (Q4 2012)</li>
</ul>

<h3>PeerConnection</h3>
<ul>
	<li>Chrome 20+ (requires about:flags tweak to enable)</li>
	<li>Targeting Chrome 22 for general availability</li>
	<li>Firefox (Q4 2012)</li>
</ul>

<h3>DataChannels</h3>
<ul>
	<li>Chrome + Firefox (Q4 2012)</li>
	<li>Internet Explorer support via ChromeFrame</li>
	<li>Mobile browser support in progress</li>
	<li>Native APIs for PeerConnection also available</li>
</ul>

<p>For more information about support for APIs such as getUserMedia, see <a href="http://caniuse.com/stream" title="caniuse.com: getUserMedia/Stream support">caniuse.com</a>,</p>

{% endblock %}
